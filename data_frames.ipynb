{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Increase compatibility with Databricks\n",
    "    from IPython.display import display as idisplay, HTML\n",
    "    displayHTML = lambda x: idisplay(HTML(x))\n",
    "    def display(*args, **kargs): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Notebook with Spark\n",
    "\n",
    "Just a quick little story.  A week or so ago (early April 2018) I had an interview with a terrific HR representative.  While I never heard back from the company, she suggested that I get Spark training because it is really a much-needed skill.  So I went exploring and found an [Udemy course by Jose Portilla](https://www.udemy.com/scala-and-spark-for-big-data-and-machine-learning/learn/v4/overview) (recommended) and a [Whizlabs course](https://www.whizlabs.com/spark-developer-certification/) (not taken yet) that would put me on the right path.  I saw a note about DataBricks and went to the end of Portilla's course where he introduced this service - I think he should have put it up front.  I love notebooks - that is how I can learn and track my notes!\n",
    "\n",
    "## Data Frames in Scala and Python\n",
    "This is my first notebook on the site.  I'm still exploring the capabilities of the Databricks notebooks, [but](https://docs.databricks.com/user-guide/notebooks/index.html#mix-languages)\n",
    "it is awesome that I can use a notebook on the cloud.\n",
    "\n",
    "This is the notebook: https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-scala.html  \n",
    "mixed (see below) with the notebook: https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html#creating-dataframes-with-python\n",
    "\n",
    "Quick notes for making notebooks:\n",
    "* The magic code \"%md\" turns a cell into a markdown cell.  \n",
    "* \"ctrl + alt + p\" inserts a cell above the current cell.  \n",
    "* \"ctrl + alt + n\" inserts a cell below the current cell.  \n",
    "* Another feature that I have never seen (well-supported) in any other notebook is the ability to switch languages: https://docs.databricks.com/user-guide/notebooks/index.html#mix-languages\n",
    "\n",
    "Use the shortcuts link to pop up all the navigation run and edit commands.\n",
    "\n",
    "Lastly - I note that this page:https://www.dezyre.com/article/top-apache-spark-certifications-to-choose-from-in-2018/348 seems to be the best for outlining how to prepare for certification and the difference between the cert tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// Create the case classes for our domain\n",
    "case class Department(id: String, name: String)\n",
    "case class Employee(firstName: String, lastName: String, email: String, salary: Int)\n",
    "case class DepartmentWithEmployees(department: Department, employees: Seq[Employee])\n",
    "\n",
    "// Create the Departments\n",
    "val department1 = new Department(\"123456\", \"Computer Science\")\n",
    "val department2 = new Department(\"789012\", \"Mechanical Engineering\")\n",
    "val department3 = new Department(\"345678\", \"Theater and Drama\")\n",
    "val department4 = new Department(\"901234\", \"Indoor Recreation\")\n",
    "\n",
    "// Create the Employees\n",
    "val employee1 = new Employee(\"michael\", \"armbrust\", \"no-reply@berkeley.edu\", 100000)\n",
    "val employee2 = new Employee(\"xiangrui\", \"meng\", \"no-reply@stanford.edu\", 120000)\n",
    "val employee3 = new Employee(\"matei\", null, \"no-reply@waterloo.edu\", 140000)\n",
    "val employee4 = new Employee(null, \"wendell\", \"no-reply@princeton.edu\", 160000)\n",
    "\n",
    "// Create the DepartmentWithEmployees instances from Departments and Employees\n",
    "val departmentWithEmployees1 = new DepartmentWithEmployees(department1, Seq(employee1, employee2))\n",
    "val departmentWithEmployees2 = new DepartmentWithEmployees(department2, Seq(employee3, employee4))\n",
    "val departmentWithEmployees3 = new DepartmentWithEmployees(department3, Seq(employee1, employee4))\n",
    "val departmentWithEmployees4 = new DepartmentWithEmployees(department4, Seq(employee2, employee3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# import pyspark class Row from module sql\n",
    "from pyspark.sql import *\n",
    "\n",
    "# Create Example Data - Departments and Employees\n",
    "\n",
    "# Create the Departments\n",
    "department1 = Row(id='123456', name='Computer Science')\n",
    "department2 = Row(id='789012', name='Mechanical Engineering')\n",
    "department3 = Row(id='345678', name='Theater and Drama')\n",
    "department4 = Row(id='901234', name='Indoor Recreation')\n",
    "\n",
    "# Create the Employees\n",
    "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
    "employee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)\n",
    "employee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\n",
    "employee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)\n",
    "employee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000)\n",
    "\n",
    "# Create the DepartmentWithEmployees instances from Departments and Employees\n",
    "departmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\n",
    "departmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])\n",
    "departmentWithEmployees3 = Row(department=department3, employees=[employee1, employee4])\n",
    "departmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])\n",
    "\n",
    "print(department1)\n",
    "print(employee2)\n",
    "print(departmentWithEmployees1.employees[0].email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the first DataFrame from a List of the Case Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "val departmentsWithEmployeesSeq1 = Seq(departmentWithEmployees1, departmentWithEmployees2)\n",
    "// Can't do a display of a Seq.  println doesn't do much with Seq.\n",
    "val df1 = departmentsWithEmployeesSeq1.toDF()\n",
    "// But display works awesome on data frames!\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%python\n",
    "departmentsWithEmployeesSeq1 = [departmentWithEmployees1, departmentWithEmployees2]\n",
    "df1 = sqlContext.createDataFrame(departmentsWithEmployeesSeq1)\n",
    "\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 2nd DataFrame from a List of Case Classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "val departmentsWithEmployeesSeq2 = Seq(departmentWithEmployees3, departmentWithEmployees4)\n",
    "val df2 = departmentsWithEmployeesSeq2.toDF()\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%python\n",
    "departmentsWithEmployeesSeq2 = [departmentWithEmployees3, departmentWithEmployees4]\n",
    "df2 = sqlContext.createDataFrame(departmentsWithEmployeesSeq2)\n",
    "\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with DataFrames\n",
    "Union 2 DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "val unionDF = df1.unionAll(df2)\n",
    "display(unionDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%python\n",
    "unionDF = df1.unionAll(df2)\n",
    "display(unionDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the dataframe to a [Parquet](http://parquet.apache.org/documentation/latest/) file.  Apparently, [Parquet](https://tech.blue-yonder.com/efficient-dataframe-storage-with-apache-parquet/) is an excellent tool for storing data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// Remove the file if it exists\n",
    "dbutils.fs.rm(\"/tmp/databricks-df-example-scala.parquet\", true)\n",
    "unionDF.write.parquet(\"/tmp/databricks-df-example-scala.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Remove the file if it exists\n",
    "dbutils.fs.rm(\"/tmp/databricks-df-example-python.parquet\", True)\n",
    "unionDF.write.parquet(\"/tmp/databricks-df-example-python.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data frame from a Parquet file.  For giggles, I attempted to swap one file to read into another, but got a conversion error.  When I wrote and read from the sayme language, things worked.  It may be that Parquet files cannot be interchanged between languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "val parquetDF = sqlContext.read.parquet(\"/tmp/databricks-df-example-scala.parquet\")\n",
    "\n",
    "display(parquetDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%python\n",
    "parquetDF = sqlContext.read.parquet(\"/tmp/databricks-df-example-python.parquet\")\n",
    "display(parquetDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Explode\" the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import org.apache.spark.sql.functions.explode\n",
    "\n",
    "// Note that the orginal code presented in the Scala notebook did not appear to work properly\n",
    "// so we adopted the code using Python as an example and figured out how to select out what we really want\n",
    "val inter = parquetDF.select(\"employees\")\n",
    "display(inter)\n",
    "val explodeDF = inter.explode($\"employees\") {\n",
    "    case Row(employee: Seq[Row]) => employee.map{ employee =>\n",
    "    val firstName = employee(0).asInstanceOf[String]\n",
    "    val lastName = employee(1).asInstanceOf[String]\n",
    "    val email = employee(2).asInstanceOf[String]\n",
    "    val salary = employee(3).asInstanceOf[Int]\n",
    "    Employee(firstName, lastName, email, salary)\n",
    "}}.select(\"firstName\", \"lastName\", \"email\", \"salary\")\n",
    "display(explodeDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df = parquetDF.select(explode(\"employees\").alias(\"e\"))\n",
    "explodeDF = df.selectExpr(\"e.firstName\", \"e.lastName\", \"e.email\", \"e.salary\")\n",
    "\n",
    "display(explodeDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ``filter()`` to return only the rows that match the given predicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "val filterDF = explodeDF\n",
    "  .filter($\"firstName\" === \"xiangrui\" || $\"firstName\" === \"michael\")\n",
    "  .sort($\"lastName\".asc)\n",
    "display(filterDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, asc\n",
    "\n",
    "# Use `|` instead of `or`\n",
    "filterDF = explodeDF.filter((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\n",
    "display(filterDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``where()`` clause is equivalent to ``filter()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "val whereDF = explodeDF.where(($\"firstName\" === \"xiangrui\") || ($\"firstName\" === \"michael\")).sort($\"lastName\".asc)\n",
    "display(whereDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%python\n",
    "whereDF = explodeDF.where((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\n",
    "display(whereDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCALA: Replace ``null`` values with ``--`` using DataFrame Na functions.  \n",
    "Python: Replace null values with ``--`` using DataFrame Na functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "val naFunctions = explodeDF.na\n",
    "val nonNullDF = naFunctions.fill(\"--\")\n",
    "display(nonNullDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%python\n",
    "nonNullDF = explodeDF.fillna(\"--\")\n",
    "display(nonNullDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve only rows with missing firstName or lastName."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "val filterNonNullDF = nonNullDF.filter($\"firstName\" === \"--\" || $\"lastName\" === \"--\").sort($\"email\".asc)\n",
    "display(filterNonNullDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%python\n",
    "filterNonNullDF = nonNullDF.filter(col(\"firstName\").like(\"--\") | col(\"lastName\").like(\"--\")).sort(\"email\")\n",
    "display(filterNonNullDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example aggregations using ``agg()`` and ``countDistinct()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// Find the distinct (firstName, lastName) combinations\n",
    "val countDistinctDF = nonNullDF.select($\"firstName\", $\"lastName\")\n",
    "  .groupBy($\"firstName\", $\"lastName\")\n",
    "  .agg(countDistinct($\"firstName\") as \"distinct_first_names\")\n",
    "display(countDistinctDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "countDistinctDF = nonNullDF.select(\"firstName\", \"lastName\")\\\n",
    "  .groupBy(\"firstName\", \"lastName\")\\\n",
    "  .agg(countDistinct(\"firstName\"))\n",
    "\n",
    "display(countDistinctDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the DataFrame and SQL Query Physical Plans (Hint: They should be the same.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "countDistinctDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%python\n",
    "countDistinctDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// register the DataFrame as a temp table so that we can query it using SQL\n",
    "nonNullDF.registerTempTable(\"databricks_df_example\")\n",
    "\n",
    "// Perform the same query as the DataFrame above and return ``explain``\n",
    "sqlContext.sql(\"\"\"\n",
    "SELECT firstName, lastName, count(distinct firstName) as distinct_first_names\n",
    "FROM databricks_df_example\n",
    "GROUP BY firstName, lastName\n",
    "\"\"\").explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# register the DataFrame as a temp table so that we can query it using SQL\n",
    "explodeDF.registerTempTable(\"databricks_df_example\")\n",
    "\n",
    "# Perform the same query as the DataFrame above and return ``explain``\n",
    "countDistinctDF_sql = sqlContext.sql(\"SELECT firstName, lastName, count(distinct firstName) as distinct_first_names FROM databricks_df_example GROUP BY firstName, lastName\")\n",
    "\n",
    "countDistinctDF_sql.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the summary statistics for the salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// Sum up all the salaries\n",
    "val salarySumDF = nonNullDF.agg(\"salary\" -> \"sum\")\n",
    "display(salarySumDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%python\n",
    "salarySumDF = explodeDF.agg({\"salary\" : \"sum\"})\n",
    "display(salarySumDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%python\n",
    "type(explodeDF.salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the summary statistics for the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "nonNullDF.describe(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%python\n",
    "nonNullDF.describe(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scala: A Utility for Flattening\n",
    "If your data has several levels of nesting, here is a helper function to flatten your DataFrame to make it easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "implicit class DataFrameFlattener(df: DataFrame) {\n",
    "  def flattenSchema: DataFrame = {\n",
    "    df.select(flatten(Nil, df.schema): _*)\n",
    "  }\n",
    "\n",
    "  protected def flatten(path: Seq[String], schema: DataType): Seq[Column] = schema match {\n",
    "    case s: StructType => s.fields.flatMap(f => flatten(path :+ f.name, f.dataType))\n",
    "    case other => col(path.map(n => s\"`$n`\").mkString(\".\")).as(path.mkString(\".\")) :: Nil\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val veryNestedDF = Seq((\"1\", (2, (3, 4)))).toDF()\n",
    "display(veryNestedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "display(veryNestedDF.flattenSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python: An example using Pandas & Matplotlib Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%python\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.clf()\n",
    "pdDF = nonNullDF.toPandas()\n",
    "pdDF.plot(x='firstName', y='salary', kind='bar', rot=45)\n",
    "display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanup: Remove the Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "dbutils.fs.rm(\"/tmp/databricks-df-example-scala.parquet\", true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%python\n",
    "dbutils.fs.rm(\"/tmp/databricks-df-example-python.parquet\", True)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
